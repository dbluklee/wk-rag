# server-rag/api/endpoints.py
"""
API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ - RAG ëª¨ë¸ë§Œ ì œê³µí•˜ë„ë¡ ìˆ˜ì •
"""
import os
import time
from typing import Dict, Any
from fastapi import HTTPException
from fastapi.responses import StreamingResponse

from .models import OllamaChatRequest, OllamaGenerateRequest
from .responses import (
    create_chat_response, create_generate_response,
    create_chat_error_response, create_generate_error_response
)
from .streaming import rag_chat_stream, rag_generate_stream

# ì „ì—­ ì±„íŒ… í•¸ë“¤ëŸ¬
chat_handler = None

def set_chat_handler(handler):
    """ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì •"""
    global chat_handler
    chat_handler = handler
    print(f"âœ… ì±„íŒ… í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")

def get_chat_handler():
    """ì±„íŒ… í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°"""
    if not chat_handler:
        raise HTTPException(status_code=503, detail="Chat handler not initialized")
    return chat_handler

async def handle_chat_request(request: OllamaChatRequest):
    """ì±„íŒ… ìš”ì²­ ì²˜ë¦¬ - RAG ëª¨ë¸ë§Œ ì§€ì›í•˜ê³  ë¡œê¹…"""
    handler = get_chat_handler()
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
    user_message = next((msg for msg in reversed(request.messages) if msg.role == "user"), None)
    if not user_message:
        raise HTTPException(status_code=400, detail="No user message found")
    
    question = user_message.content
    
    try:
        # RAG ëª¨ë¸ì¸ ê²½ìš°ë§Œ RAG ì²˜ë¦¬ + ë¡œê¹…
        if request.model == handler.rag_model_name:
            if request.stream:
                return StreamingResponse(
                    rag_chat_stream(handler, question, request.model),
                    media_type="application/x-ndjson"
                )
            else:
                # RAG ì²˜ë¦¬ (ë¡œê¹… í¬í•¨)
                response_content = await handler.process_with_rag(question)
                return create_chat_response(request.model, response_content)
        else:
            # ì¼ë°˜ LLM ëª¨ë¸ì¸ ê²½ìš°: í”„ë¡ì‹œë§Œ í•˜ê³  ë¡œê¹… ì•ˆí•¨
            print(f"ğŸ”„ ì¼ë°˜ LLM ëª¨ë¸ í”„ë¡ì‹œ (ë¡œê¹… ì•ˆí•¨): {request.model}")
            
            # LLM ì„œë²„ë¡œ ì§ì ‘ í”„ë¡ì‹œ (ë¡œê¹… ì—†ìŒ)
            import requests
            try:
                response = requests.post(
                    f"{handler.llm_server_url}/api/chat",
                    json=request.dict(),
                    timeout=120
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    return create_chat_error_response(
                        request.model, 
                        f"LLM server error: {response.status_code}"
                    )
            except Exception as e:
                return create_chat_error_response(
                    request.model, 
                    f"Proxy error: {str(e)}"
                )
            
    except Exception as e:
        print(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return create_chat_error_response(request.model, str(e))

async def handle_generate_request(request: OllamaGenerateRequest):
    """ìƒì„± ìš”ì²­ ì²˜ë¦¬ - RAG ëª¨ë¸ë§Œ ì§€ì›"""
    handler = get_chat_handler()
    
    try:
        # RAG ëª¨ë¸ë§Œ ì§€ì›
        if request.model == handler.rag_model_name:
            if request.stream:
                return StreamingResponse(
                    rag_generate_stream(handler, request.prompt, request.model),
                    media_type="application/x-ndjson"
                )
            else:
                response_content = await handler.process_with_rag(request.prompt)
                return create_generate_response(request.model, response_content)
        else:
            # RAG ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš° ì˜¤ë¥˜ ì‘ë‹µ
            return create_generate_error_response(
                request.model,
                f"Model '{request.model}' not supported. Only '{handler.rag_model_name}' is available on this RAG server."
            )
            
    except Exception as e:
        print(f"âŒ ìƒì„± ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return create_generate_error_response(request.model, str(e))

def get_model_list() -> Dict[str, Any]:
    """ëª¨ë¸ ëª©ë¡ ìƒì„± - RAG ëª¨ë¸ë§Œ ì œê³µ"""
    rag_model_name = os.environ.get("RAG_MODEL_NAME", "rag-cheeseade:latest")
    
    # RAG ëª¨ë¸ë§Œ í¬í•¨
    models = [
        {
            "name": rag_model_name,
            "model": rag_model_name,
            "modified_at": "2024-12-01T00:00:00.000000000Z",
            "size": 2500000000,
            "digest": f"sha256:{abs(hash(rag_model_name)):064x}",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "rag-enhanced",
                "families": ["rag-enhanced"],
                "parameter_size": "RAG+27B",
                "quantization_level": "Q4_K_M"
            }
        }
    ]
    
    return {"models": models}

def get_health_status() -> Dict[str, Any]:
    """í—¬ìŠ¤ì²´í¬ ìƒíƒœ ìƒì„±"""
    handler_status = "initialized" if chat_handler else "not_initialized"
    rag_model = os.environ.get("RAG_MODEL_NAME", "unknown")
    
    return {
        "status": "healthy" if chat_handler else "degraded",
        "service": "cheeseade-rag-server",
        "timestamp": int(time.time()),
        "chat_handler": handler_status,
        "models": {
            "rag_model": rag_model,
            "supported_models": [rag_model],
            "total_models": 1
        }
    }