from typing import List, Dict, Any, Optional
from langchain_milvus import Milvus
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_core.documents import Document
from langchain.vectorstores.base import VectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection


class MilvusVectorStore(VectorStore):
    def __init__(self, 
                 collection_name: str,
                 embedding_model: HuggingFaceEmbeddings,
                 metric_type: str = 'IP',
                 index_type: str = 'HNSW',
                 milvus_host: str = 'localhost',
                 milvus_port: str = '19530',
                 always_new: bool = True):
        """
        Milvus Vector Store for LangChain
        
        Args:
            collection_name: Milvus ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: ì„ë² ë”© ìƒì„±ìš© ëª¨ë¸
            milvus_host: Milvus ì„œë²„ í˜¸ìŠ¤íŠ¸
            milvus_port: Milvus ì„œë²„ í¬íŠ¸
        """
        self.collection_name = collection_name
        self.embedding_model = embedding_model 
        self.embedding_dim = 1024  # BAAI/bge-m3 ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›
        self.always_new = always_new
        self.metric_type = metric_type
        self.index_type = index_type

        
        # Milvus ì—°ê²°
        print("\nMilvus ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤\n")
        connections.connect("default", host=milvus_host, port=milvus_port)

        # 2. ì„œë²„ ë²„ì „ ì •ë³´ë¥¼ ìš”ì²­í•˜ì—¬ ì‹¤ì œ í†µì‹  í™•ì¸
        server_version = utility.get_server_version()
        print(f"\nâœ… Milvus ì—°ê²° ì„±ê³µ! (ì„œë²„ ë²„ì „: {server_version})\n")
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self._setup_collection()

    def _setup_collection(self):
        """Milvus ì»¬ë ‰ì…˜ ì„¤ì •"""
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        print("\nìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•©ë‹ˆë‹¤\n")
        fields = [
            # id í•„ë“œ (ìë™ ID ìƒì„±)
            FieldSchema(name="pk", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),
            # ë²¡í„°ë¥¼ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            # Header 1ì„ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="header1", dtype=DataType.VARCHAR, max_length=200),
            # Header 2ì„ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="header2", dtype=DataType.VARCHAR, max_length=200),
            # sourceë¥¼ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=100),
            # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  í•„ë“œ
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535)
        ]
        
        schema = CollectionSchema(fields, f"'{self.collection_name}' Feature Document")
        
        if self.always_new == True:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
            if utility.has_collection(self.collection_name):
                print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}'ì„ ì‚­ì œí•©ë‹ˆë‹¤.")
                utility.drop_collection(self.collection_name)


        # ì»¬ë ‰ì…˜ ìƒì„±
        try:
            self.collection = Collection(self.collection_name, schema)
            print(f"\nâœ…ìƒˆ ì»¬ë ‰ì…˜ '{self.collection_name}'ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n")
        except Exception:
            self.collection = Collection(self.collection_name)
            print(f"\nâœ…ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}'ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self._create_index()

    def _create_index(self):
        
        if self.index_type == 'HNSW':
            params = {"M": 8, "efConstruction": 64}
        elif self.index_type in ["IVF_FLAT", "IVF_SQ8", "IVF_PQ"]:
            params = {"nlist": 128}
        else:
            params = {}

        """ë²¡í„° í•„ë“œì— ì¸ë±ìŠ¤ ìƒì„±"""
        index_params = {
            "metric_type": self.metric_type,  
            "index_type": self.index_type,
            "params": params
        }
        
        try:
            print(f"\n'{self.collection_name}' ì»¬ë ‰ì…˜ì— ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\n")
            self.collection.create_index("vector", index_params)
            print(f"\nâœ… ì¸ë±ìŠ¤ ìƒì„± ë° ì™„ë£Œ.(metric_type: {self.metric_type}, index_type: {self.index_type}, params: {params})\n")
        except Exception as e:
            print(f"\nâŒì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}\n")



    # vector_db/milvus.pyì˜ add_texts ë©”ì„œë“œ ìˆ˜ì • (ë°°ì¹˜ ì²˜ë¦¬)

    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None, **kwargs) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
        """
        if metadatas is None:
            metadatas = [{}] * len(texts)
        
        print(f"\nğŸ“¤ {len(texts)}ê°œ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
        BATCH_SIZE = 16  # í•œ ë²ˆì— 16ê°œì”© ì²˜ë¦¬
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        all_vectors = []
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i+BATCH_SIZE]
            print(f"   ë°°ì¹˜ {i//BATCH_SIZE + 1}/{(len(texts)-1)//BATCH_SIZE + 1}: {len(batch_texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
            
            try:
                # ë°°ì¹˜ë³„ ì„ë² ë”© ìƒì„±
                batch_vectors = self.embedding_model.embed_documents(batch_texts)
                all_vectors.extend(batch_vectors)
                print(f"   âœ… ë°°ì¹˜ ì™„ë£Œ ({len(batch_vectors)}ê°œ ë²¡í„° ìƒì„±)")
                
            except RuntimeError as e:
                if "CUDA" in str(e):
                    print(f"   âŒ CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ë°œìƒ, ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„...")
                    # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
                    for j in range(i, min(i+BATCH_SIZE, len(texts))):
                        single_vector = self.embedding_model.embed_documents([texts[j]])
                        all_vectors.extend(single_vector)
                        print(f"     ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬: {j+1}/{len(texts)}")
                else:
                    raise e
        
        print(f"âœ… ì „ì²´ {len(all_vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ")
        
        # ë°ì´í„° ì¤€ë¹„
        header1s = []
        header2s = []
        sources = []
        contents = []
        
        for i, text in enumerate(texts):
            metadata = metadatas[i]
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            header1 = metadata.get('Header 1', '')
            header2 = metadata.get('Header 2', '')
            source = metadata.get('source', '')

            # ë°ì´í„° ì¶”ê°€
            header1s.append(header1)
            header2s.append(header2)
            sources.append(source)
            contents.append(text)
        
        # Milvusì— ì‚½ì…í•  ë°ì´í„° êµ¬ì„±
        data = [
            all_vectors,  # ë°°ì¹˜ë¡œ ìƒì„±ëœ ì „ì²´ ë²¡í„°
            header1s,
            header2s,
            sources,
            contents
        ]

        # ì»¬ë ‰ì…˜ ë¡œë“œ (ê²€ìƒ‰ì„ ìœ„í•´ í•„ìš”)
        self.collection.load()
        
        # ë°ì´í„° ì‚½ì…
        mr = self.collection.insert(data)
        print(f"\nâœ… {len(texts)}ê°œ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
        
        # ë°ì´í„° í”ŒëŸ¬ì‹œ (ì˜êµ¬ ì €ì¥)
        self.collection.flush()
        print("\nâœ… ë°ì´í„°ê°€ ì˜êµ¬ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
        
        return mr.primary_keys  


    def add_documents(self, documents: List[Document], **kwargs) -> List[str]:
        """
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        
        Args:
            documents: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶”ê°€ëœ ë¬¸ì„œë“¤ì˜ ID ë¦¬ìŠ¤íŠ¸
        """
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        return self.add_texts(texts, metadatas, **kwargs)

    
    # vector_db/milvus.pyì˜ similarity_search ë©”ì„œë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •

    def similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
        ) -> List[Document]:
        """
        ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (LangChain ì¸í„°í˜ì´ìŠ¤)
        """
        # ë¨¼ì € ì»¬ë ‰ì…˜ì˜ ì´ ë¬¸ì„œ ìˆ˜ í™•ì¸
        self.collection.load()
        total_docs = self.collection.num_entities
        print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ì´ ë¬¸ì„œ ìˆ˜: {total_docs}")
        print(f"ğŸ“Š ìš”ì²­ëœ k ê°’: {k}")
        
        # ì‹¤ì œ k ê°’ ì¡°ì • (ì´ ë¬¸ì„œ ìˆ˜ë³´ë‹¤ í´ ìˆ˜ ì—†ìŒ)
        actual_k = min(k, total_docs)
        print(f"ğŸ“Š ì‹¤ì œ ê²€ìƒ‰í•  k ê°’: {actual_k}")
        
        if total_docs == 0:
            print("âš ï¸ ì»¬ë ‰ì…˜ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        print(f"\nğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: '{query}'")
        query_vector = self.embedding_model.embed_query(query)
        print(f"ğŸ“ ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: {len(query_vector)}")

        # ë²¡í„° í•„ë“œì— ëŒ€í•œ ì¸ë±ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        vector_index = self.collection.indexes[0]
        index_type = vector_index.params.get("index_type")
        metric_type = vector_index.params.get("metric_type")

        if index_type == 'HNSW':
            params = {"ef": 64}
        elif index_type in ["IVF_FLAT", "IVF_SQ8", "IVF_PQ"]:
            params = {"nprobe": 10}
        else:
            params = {}

        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        print(f"\nğŸ”§ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°:")
        print(f"   - metric_type: {metric_type}")
        print(f"   - index_type: {index_type}")
        print(f"   - params: {params}")
        print(f"   - limit: {actual_k}")
        
        search_params = {"metric_type": metric_type, "params": params}
        
        # ê²€ìƒ‰ ì‹¤í–‰
        print(f"\nğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        try:
            results = self.collection.search(
                data=[query_vector],
                anns_field="vector",
                param=search_params,
                limit=actual_k,
                output_fields=["header1", "header2", "source", "content"]
            )
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜: {len(results[0]) if results else 0}")
            
            # ê° ê²°ê³¼ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥
            if results and len(results[0]) > 0:
                for i, hit in enumerate(results[0]):
                    print(f"   ê²°ê³¼ {i+1}: score={hit.score:.4f}, id={hit.id}")
                    print(f"          header2: {hit.entity.get('header2', 'N/A')}")
                    print(f"          content ê¸¸ì´: {len(hit.entity.get('content', ''))}")
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
        
        # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(f"\nğŸ”„ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        docs = []
        for hits in results:
            for hit in hits:
                doc = Document(
                    page_content=hit.entity.get("content"),
                    metadata={
                        "Header 1": hit.entity.get("header1"),
                        "Header 2": hit.entity.get("header2"),
                        "source": hit.entity.get("source"),
                        "score": hit.score,
                        "id": hit.id
                    }
                )
                docs.append(doc)
        
        print(f"âœ… {len(docs)}ê°œ ë¬¸ì„œë¥¼ LangChain Documentë¡œ ë³€í™˜ ì™„ë£Œ")
        return docs

    
    def similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
        ) -> List[tuple]:
        """ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        docs = self.similarity_search(query, k, **kwargs)
        print("\nâœ… ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨.\n")
        return [(doc, doc.metadata.get('score', 0.0)) for doc in docs]





    @classmethod
    def from_texts(cls, texts: List[str], embedding_model: HuggingFaceEmbeddings, metadatas: Optional[List[dict]] = None, **kwargs):
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        vector_store = cls(embedding_model=embedding_model, **kwargs)
        vector_store.add_texts(texts, metadatas)
        print("\nâœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ.\n")
        return vector_store

    @classmethod
    def from_documents(cls, documents: List[Document], embedding_model: HuggingFaceEmbeddings, **kwargs):
        """Document ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        vector_store = cls(embedding_model=embedding_model, **kwargs)
        vector_store.add_documents(documents)
        print("\nâœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ.\n")
        return vector_store

 








