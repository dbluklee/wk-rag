from langchain_text_splitters import MarkdownHeaderTextSplitter
from typing import List
import os
import glob

def load_md_from_docs():
    """docs í´ë”ì—ì„œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ ì°¾ê¸°"""
    docs_path = "./docs"
    
    if not os.path.exists(docs_path):
        print("âŒ docs í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    md_files = glob.glob(os.path.join(docs_path, "*.md"))
    
    if len(md_files) == 0:
        print("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ“ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {[os.path.basename(f) for f in md_files]}")
    return md_files

def save_markdown_chunks_to_file(chunks: List, output_file: str = "./chunking/chunks/markdown_chunks_output.txt"):
    """ë§ˆí¬ë‹¤ìš´ ì²­í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    if not chunks:
        print("âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ ê²°ê³¼ - ì´ {len(chunks)}ê°œ ì²­í¬\n")
            f.write("=" * 80 + "\n\n")
            
            for i, chunk in enumerate(chunks, 1):
                f.write(f"=== MARKDOWN CHUNK {i} ===\n")
                f.write(f"Source: {chunk.metadata.get('source', 'Unknown')}\n")
                
                # í—¤ë” ì •ë³´ ì¶œë ¥
                header1 = chunk.metadata.get('Header 1', '')
                header2 = chunk.metadata.get('Header 2', '')
                
                if header1:
                    f.write(f"Header 1: {header1}\n")
                if header2:
                    f.write(f"Header 2: {header2}\n")
                
                f.write("\nPAGE_CONTENT:\n")
                f.write(f"{chunk.page_content}\n")
                f.write("\n" + "=" * 80 + "\n\n")
        
        print(f"ğŸ’¾ ë§ˆí¬ë‹¤ìš´ ì²­í¬ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

def process_single_markdown_file(file_path: str) -> List:
    """ë‹¨ì¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²­í‚¹"""
    filename = os.path.basename(file_path)
    print(f"ğŸ“– ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬: {filename}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            markdown_text = f.read()
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []

    # ë¶„í•  ê¸°ì¤€ì´ ë  í—¤ë”ë¥¼ ì •ì˜
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
    ]

    # MarkdownHeaderTextSplitter ì´ˆê¸°í™”
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on, 
        return_each_line=False
    )

    # í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤í–‰
    print(f"âœ‚ï¸ ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  ì¤‘...")
    initial_chunks = markdown_splitter.split_text(markdown_text)
    print(f"ğŸ“Š ì´ˆê¸° ë¶„í•  ê²°ê³¼: {len(initial_chunks)}ê°œ ì²­í¬")

    # ë©”íƒ€ë°ì´í„° í›„ì²˜ë¦¬
    processed_chunks = []
    for i, chunk in enumerate(initial_chunks):
        chunk.metadata['source'] = filename
        
        # Header 2ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ feature ì¶”ê°€
        header2 = chunk.metadata.get('Header 2', '')
        if header2:
            new_page_content = f'\n---\nfeature: {header2}\n{chunk.page_content}'
        else:
            new_page_content = f'\n---\nfeature: Unknown\n{chunk.page_content}'
            
        chunk.page_content = new_page_content
        processed_chunks.append(chunk)

    print(f"âœ… {filename}: {len(processed_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    return processed_chunks

def chunk_markdown_files() -> List:
    print(f"\nğŸ“ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²­í‚¹ ì‹œì‘...")
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì°¾ê¸°
    md_files = load_md_from_docs()
    
    if not md_files:
        print("âŒ ì²˜ë¦¬í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    all_chunks = []
    
    # ê° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì²˜ë¦¬
    for file_path in md_files:
        chunks = process_single_markdown_file(file_path)
        all_chunks.extend(chunks)
    
    print(f"âœ… ì´ {len(all_chunks)}ê°œ ë§ˆí¬ë‹¤ìš´ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    
    # íŒŒì¼ë¡œ ì €ì¥
    if all_chunks:
        save_markdown_chunks_to_file(all_chunks)
    
    return all_chunks
