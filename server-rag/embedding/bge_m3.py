import torch
from langchain_huggingface import HuggingFaceEmbeddings
import os
import subprocess
import sys
from pathlib import Path

def download_model_automatically(model_path: str, model_name: str = "BAAI/bge-m3") -> bool:
    """
    ìë™ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        print(f"ğŸš€ ìë™ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        print(f"   ëª¨ë¸: {model_name}")
        print(f"   ì €ì¥ ìœ„ì¹˜: {model_path}")
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(model_path).mkdir(parents=True, exist_ok=True)
        
        # HuggingFace Hub ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
        try:
            from huggingface_hub import snapshot_download
            print("ğŸ“¥ HuggingFace Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            print("   âš ï¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ì…ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            downloaded_path = snapshot_download(
                repo_id=model_name,
                local_dir=model_path,
                local_dir_use_symlinks=False,
                resume_download=True
            )
            
            print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_path}")
            return True
            
        except ImportError:
            print("âŒ huggingface_hubê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install huggingface_hubë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return False
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ ìë™ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def verify_model_files(model_path: str) -> bool:
    """
    ëª¨ë¸ íŒŒì¼ì´ ì™„ì „íˆ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    required_files = ["config.json"]
    optional_files = ["pytorch_model.bin", "model.safetensors"]
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    for file in required_files:
        if not Path(model_path + "/" + file).exists():
            return False
    
    # ëª¨ë¸ íŒŒì¼ ì¤‘ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
    has_model_file = any(Path(model_path + "/" + file).exists() for file in optional_files)
    if not has_model_file:
        return False
    
    return True

def get_bge_m3_model() -> HuggingFaceEmbeddings:
    """
    BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ë¡œì»¬ ëª¨ë¸ ìš°ì„ , ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
    USE_CUDA í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ CPU/GPUë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    
    # USE_CUDA í™˜ê²½ë³€ìˆ˜ í™•ì¸
    use_cuda = os.getenv('USE_CUDA', 'true').lower() == 'true'
    
    # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    local_model_path = "/app/embedding/models/bge-m3"
    huggingface_model_name = 'BAAI/bge-m3'
    
    # ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    local_model_exists = verify_model_files(local_model_path)
    
    # ëª¨ë¸ ì„ íƒ ë¡œì§
    if local_model_exists:
        print(f"ğŸ¯ ë¡œì»¬ ëª¨ë¸ ë°œê²¬: {local_model_path}")
        model_name = local_model_path
        
        # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì •ë³´ ì¶œë ¥
        try:
            model_files = list(Path(local_model_path).glob("*"))
            total_size = sum(f.stat().st_size for f in model_files if f.is_file()) / (1024*1024*1024)
            print(f"ğŸ“ ë¡œì»¬ ëª¨ë¸ í¬ê¸°: {total_size:.1f}GB")
            print(f"ğŸ“‹ ëª¨ë¸ íŒŒì¼ ìˆ˜: {len([f for f in model_files if f.is_file()])}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            
    else:
        print(f"âš ï¸ ë¡œì»¬ ëª¨ë¸ ì—†ìŒ: {local_model_path}")
        
        # ìë™ ë‹¤ìš´ë¡œë“œ ì‹œë„
        print(f"ğŸ¤– ìë™ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
        
        if download_model_automatically(local_model_path, huggingface_model_name):
            # ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì‹œ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            if verify_model_files(local_model_path):
                print(f"âœ… ìë™ ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©")
                model_name = local_model_path
            else:
                print(f"âš ï¸ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ íŒŒì¼ ë¶ˆì™„ì „, HuggingFace Hub ì‚¬ìš©")
                model_name = huggingface_model_name
        else:
            # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ HuggingFace Hub ì§ì ‘ ì‚¬ìš©
            print(f"âš ï¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, HuggingFace Hubì—ì„œ ì§ì ‘ ë¡œë“œ")
            print(f"ğŸŒ ëª¨ë¸ ì†ŒìŠ¤: {huggingface_model_name}")
            model_name = huggingface_model_name
    
    # USE_CUDAê°€ falseë©´ CUDA_VISIBLE_DEVICESë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •í•˜ì—¬ GPU ë¹„í™œì„±í™”
    if not use_cuda:
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸ”§ USE_CUDA=false - GPU ë¹„í™œì„±í™”, CPU ëª¨ë“œë¡œ ì „í™˜")
        device = 'cpu'
    elif torch.cuda.is_available():
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_free = torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)
            gpu_memory_gb = gpu_memory / (1024**3)
            gpu_free_gb = gpu_free / (1024**3)
            
            print(f"ğŸ” GPU ì „ì²´ ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB")
            print(f"ğŸ” GPU ì—¬ìœ  ë©”ëª¨ë¦¬: {gpu_free_gb:.1f}GB")
            
            # ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ 2GB ë¯¸ë§Œì´ë©´ ì—ëŸ¬ ë°œìƒ
            if gpu_free_gb < 2.0:
                raise RuntimeError(
                    f"âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ì—¬ìœ  ë©”ëª¨ë¦¬: {gpu_free_gb:.1f}GB < 2.0GB í•„ìš”\n"
                    f"í•´ê²°ë°©ë²•:\n"
                    f"1. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n"
                    f"2. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©\n"
                    f"3. ë” í° GPU ë©”ëª¨ë¦¬ í™˜ê²½ ì‚¬ìš©"
                )
            else:
                device = 'cuda'
        except Exception as e:
            # GPU ìƒíƒœ í™•ì¸ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ CPUë¡œ ì „í™˜
            if "CUDA" in str(e) or "GPU" in str(e):
                raise RuntimeError(
                    f"âŒ GPU í™•ì¸ ì‹¤íŒ¨: {e}\n"
                    f"í•´ê²°ë°©ë²•:\n"
                    f"1. NVIDIA ë“œë¼ì´ë²„ í™•ì¸\n" 
                    f"2. CUDA ì„¤ì¹˜ í™•ì¸\n"
                    f"3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©"
                )
            else:
                raise e
    else:
        raise RuntimeError(
            "âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\n"
            "í•´ê²°ë°©ë²•:\n"
            "1. NVIDIA GPU ë° ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸\n"
            "2. CUDA ì„¤ì¹˜ í™•ì¸\n" 
            "3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‚¬ìš©"
        )
    
    print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ“ ëª¨ë¸ ì†ŒìŠ¤: {'ë¡œì»¬ íŒŒì¼' if model_name == local_model_path else 'HuggingFace Hub'}")
    
    model_kwargs = {'device': device}
    encode_kwargs = {'normalize_embeddings': True}
    
    try:
        print(f"â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        
        # ë¡œë”© ì„±ê³µ í›„ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_embedding = embeddings.embed_query("test")
        embedding_dim = len(test_embedding)
        
        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {embedding_dim}")
        print(f"   ğŸ¯ ë””ë°”ì´ìŠ¤: {device}")
        print(f"   ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_name}")
        
        return embeddings
        
    except Exception as e:
        error_msg = f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\n"
        
        if model_name == local_model_path:
            error_msg += "í•´ê²°ë°©ë²•:\n"
            error_msg += "1. ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì†ìƒ í™•ì¸\n"
            error_msg += "2. ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ: ./embedding/download-models.sh -f\n"
            error_msg += "3. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‹œë„\n"
            error_msg += "4. HuggingFace Hub ëª¨ë“œë¡œ ì „í™˜ (ë¡œì»¬ ëª¨ë¸ ì‚­ì œ)"
        else:
            error_msg += "í•´ê²°ë°©ë²•:\n"
            error_msg += "1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)\n"
            error_msg += "2. ë¡œì»¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ./embedding/download-models.sh\n"
            error_msg += "3. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸\n"
            error_msg += "4. USE_CUDA=falseë¡œ ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ì‹œë„"
        
        raise RuntimeError(error_msg)